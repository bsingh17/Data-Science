{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Income Prediction**\n",
    "\n",
    "**Objective :** Explore and implement **Principal Component Analysis** - a **Dimensionality Reduction** technique with Logistic Regression, to determine whether a person makes over 50K a year.\n",
    "\n",
    "For this project we will be using the following UCI dataset- https://archive.ics.uci.edu/ml/datasets/Adult\n",
    "\n",
    "Here are the features represented through columns :\n",
    "<br>\n",
    "\n",
    "**Input variables**\n",
    "<br>\n",
    "1 - age \n",
    "<br>\n",
    "2 - workclass\n",
    "<br>\n",
    "3 - fnlwgt\n",
    "<br>\n",
    "4 - education \n",
    "<br>\n",
    "5 - education-num\n",
    "<br>\n",
    "6 - marital-status\n",
    "<br>\n",
    "7 - occupation\n",
    "<br>\n",
    "8 - relationship\n",
    "<br>\n",
    "9 - race\n",
    "<br>\n",
    "10 - sex\n",
    "<br>\n",
    "11 - capital-gain\n",
    "<br>\n",
    "12 - capital-loss\n",
    "<br>\n",
    "13 - hours-per-week\n",
    "<br>\n",
    "14 - native-country\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "**Output/Target Variable**\n",
    "<br>\n",
    "15 - income\n",
    "- (>)50K\n",
    "- (<=)50K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "-\tImport Python libraries\n",
    "-\tImport dataset\n",
    "-\tExploratory data analysis\n",
    "-\tSplit data into training and test set\n",
    "-\tFeature engineering\n",
    "-\tFeature scaling\n",
    "-\tLogistic regression model with all features\n",
    "-\tLogistic Regression with PCA\n",
    "-\tSelect right number of dimensions\n",
    "-\tPlot explained variance ratio with number of dimensions\n",
    "-\tConclusion\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#import numpy\n",
    "import numpy as np \n",
    "\n",
    "#import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#import libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "\n",
    "Use pandas to read adult.csv as a dataframe called adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_csv('adult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check shape of dataset\n",
    "<br>\n",
    "Use .shape() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many instances and attributes are present in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Dataset\n",
    "<br>\n",
    "Use head() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View summary of dataframe\n",
    "<br>\n",
    "Use info() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the dataset shows that there are no missing values. But the preview shows that the dataset contains values coded as `?`. So, we will encode `?` as NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode `?` as `NaNs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[adult == '?'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again check the summary of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which variables contain missing values?\n",
    "<br>\n",
    "What is the datatype of these variables?\n",
    "<br>\n",
    "We will impute the missing values with the most frequent value - the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['workclass', 'occupation', 'native.country']:\n",
    "    adult[col].fillna(adult[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check again for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting feature vector and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the 'income' column to y\n",
    "y = df['income']\n",
    "\n",
    "#Drop the 'income' column from the dataframe and set the remaining dataframe to X\n",
    "X = df.drop(['income'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into separate training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the data set into training data and testing data in a 7:3 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import LabelEncoder from sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Create a list named 'categorical' of all the categorical features in X\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "\n",
    "#Build a for loop that traverses through the list 'categorical' that you created above. \n",
    "#For each iteration of the loop,\n",
    "#Create an instance of LabelEncoder named 'encoder'\n",
    "#Use .fit_transform method to fit encoder to current feature in X_train and X_test\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Create an instance named 'scaler'\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Use .fit_transform method to scale ALL features in X_train and X_test\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import LogisticRegression\n",
    "#Import import accuracy_score from sklearn.metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Create an instance of LogisticRegression() called logreg and fit it to the training data.\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Create predictions from the test set and name the result y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#print out the accuracy score for LogisticRegression\n",
    "print('Logistic Regression accuracy score with all the features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with PCA\n",
    "\n",
    "Scikit-Learn's PCA class implements PCA algorithm using the code below. Before diving deep, we will explain another important concept called explained variance ratio.\n",
    "\n",
    "\n",
    "### Explained Variance Ratio\n",
    "\n",
    "A very useful piece of information is the **explained variance ratio** of each principal component. It is available via the `explained_variance_ratio_ ` variable. It indicates the proportion of the dataset’s variance that lies along the axis of each principal component.\n",
    "\n",
    "Now, let's get to the PCA implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PCA from sklearn.decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Create an instance named 'pca'\n",
    "pca = PCA()\n",
    "\n",
    "#Use .fit_transform method to fit pca to X_train\n",
    "X_train = pca.fit_transform(X_train)\n",
    "\n",
    "#Use pca.explained_variance_ratio_ to find out feature-wise proportion of the dataset’s variance\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- Approximately what % of variance is explained by the first 13 variables?\n",
    "\n",
    "- How much variance is explained by the last variable? Can we assume that it carries little information? \n",
    "\n",
    "- Let's now drop it, train the model again and calculate the accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with first 13 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the 'income' column to y\n",
    "y = df['income']\n",
    "\n",
    "#Drop the 'income' and 'native.country' columns from the dataframe and set the remaining dataframe to X\n",
    "X = df.drop(['income','native.country'], axis=1)\n",
    "\n",
    "#Split the data set into training data and testing data in a 7:3 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "#Create a list named 'categorical' of all the categorical features in our newly created X\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\n",
    "\n",
    "\n",
    "#Build a for loop that traverses through the list 'categorical' that you created above. \n",
    "#For each iteration of the loop,\n",
    "#Create an instance of LabelEncoder named 'encoder'\n",
    "#Use .fit_transform method to fit encoder to current feature in X_train and X_test\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "#Use .fit_transform method to scale ALL features in X_train and X_test\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "#Create an instance of LogisticRegression() called logreg and fit it to the training data.\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Create predictions from the test set and name the result y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#print out the accuracy score for LogisticRegression\n",
    "print('Logistic Regression accuracy score with the first 13 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- What is the change in accuracy of our model?\n",
    "\n",
    "- Now, consider the last two features. Approximately what % of variance is explained by them, combined?\n",
    "\n",
    "- Let's drop them both, train the model again and calculate the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with first 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the 'income' column to y\n",
    "y = df['income']\n",
    "\n",
    "#Drop the 'income','native.country', 'hours.per.week' columns from the dataframe and set the remaining dataframe to X\n",
    "X = df.drop(['income','native.country', 'hours.per.week'], axis=1)\n",
    "\n",
    "#Split the data set into training data and testing data in a 7:3 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "#Create a list named 'categorical' of all the categorical features in our newly created X\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\n",
    "\n",
    "\n",
    "#Build a for loop that traverses through the list 'categorical' that you created above. \n",
    "#For each iteration of the loop, \n",
    "#Create an instance of LabelEncoder named 'encoder'\n",
    "#Use .fit_transform method to fit encoder to current feature in X_train and X_test\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "\n",
    "#Use .fit_transform method to scale ALL features in X_train and X_test\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "#Create an instance of LogisticRegression() called logreg and fit it to the training data.\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Create predictions from the test set and name the result y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "#print out the accuracy score for LogisticRegression\n",
    "print('Logistic Regression accuracy score with the first 12 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- What is the change in accuracy of our model, if it is trained with 12 features?\n",
    "\n",
    "- Lastly, we will take the last three features combined. Approximately what % of variance is explained by them?\n",
    "\n",
    "- Let's repeat the process, drop these features, train the model again and calculate the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with first 11 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the 'income' column to y\n",
    "y = df['income']\n",
    "\n",
    "#Drop the 'income','native.country','hours.per.week','capital.loss' columns from the dataframe and set the remaining dataframe to X\n",
    "X = df.drop(['income','native.country', 'hours.per.week', 'capital.loss'], axis=1)\n",
    "\n",
    "#Split the data set into training data and testing data in a 7:3 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "#Create a list named 'categorical' of all the categorical features in our newly created X\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\n",
    "\n",
    "\n",
    "#Build a for loop that traverses through the list 'categorical' that you created above. \n",
    "#For each iteration of the loop, \n",
    "#Create an instance of LabelEncoder named 'encoder'\n",
    "#Use .fit_transform method to fit encoder to current feature in X_train and X_test\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "#Use .fit_transform method to scale ALL features in X_train and X_test\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "#Create an instance of LogisticRegression() called logreg and fit it to the training data.\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Create predictions from the test set and name the result y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "#print out the accuracy score for LogisticRegression\n",
    "print('Logistic Regression accuracy score with the first 11 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Has the accuracy increased or decreased if we drop the last three features?\n",
    "\n",
    "- Our aim is to maximize the accuracy. When did we get the highest accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select right number of dimensions\n",
    "\n",
    "- The above process works well if the number of dimensions are small.\n",
    "\n",
    "- But, it is quite cumbersome if we have large number of dimensions.\n",
    "\n",
    "- In that case, a better approach is to compute the number of dimensions that can explain significantly large portion of the variance.\n",
    "\n",
    "- The following code computes PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 90% of the training set variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the 'income' column to y\n",
    "y = df['income']\n",
    "\n",
    "#Drop the 'income' column from the dataframe and set the remaining dataframe to X\n",
    "X = df.drop(['income'], axis=1)\n",
    "\n",
    "#Split the data set into training data and testing data in a 7:3 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "#Create a list named 'categorical' of all the categorical features in X\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "\n",
    "\n",
    "#Build a for loop that traverses through the list 'categorical' that you created above. \n",
    "#For each iteration of the loop,\n",
    "#Create an instance of LabelEncoder named 'encoder'\n",
    "#Use .fit_transform method to fit encoder to current feature in X_train \n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "#Use .fit_transform method to scale ALL features in X_train\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "#Create an instance of PCA named 'pca'\n",
    "pca= PCA()\n",
    "\n",
    "#Use .fit_transform method to fit pca to X_train\n",
    "pca.fit(X_train)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "dim = np.argmax(cumsum >= 0.90) + 1\n",
    "print('The number of dimensions required to preserve 90% of variance is',dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- With the required number of dimensions found, we can then set number of dimensions to `dim` and run PCA again.\n",
    "\n",
    "- With the number of dimensions set to `dim`, we can then calculate the required accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot explained variance ratio with number of dimensions\n",
    "\n",
    "- An alternative option is to plot the explained variance as a function of the number of dimensions.\n",
    "\n",
    "- In the plot, we should look for an elbow where the explained variance stops growing fast.\n",
    "\n",
    "- This can be thought of as the intrinsic dimensionality of the dataset.\n",
    "\n",
    "- Now, we will plot cumulative explained variance ratio with number of components to show how variance ratio varies with number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlim(0,14,1)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "According to the above plot, how many components explain around 90% of variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "-\tIn this project, we discussed Principal Component Analysis – the most popular dimensionality reduction technique.\n",
    "-\tWe demonstrated PCA implementation with Logistic Regression on the adult dataset.\n",
    "-\tMaximum accuracy was first found through a manual feature selection process. \n",
    "-\tAs expected, the number of dimensions required to preserve 90 % of variance matched.\n",
    "-\tFinally, we plotted the explained variance ratio with number of dimensions. The graph confirmed our findings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
