{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEPS:\n",
    "* Step 1) Basic operation with PySpark\n",
    "* Step 2) Data preprocessing\n",
    "* Step 3) Build a data processing pipeline\n",
    "* Step 4) Build the classifier\n",
    "* Step 5) Train and evaluate the model\n",
    "* Step 6) Tune the hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Initialize the SPARKContext and  SQLContext**\n",
    "SQLContext allows connecting the engine with different data sources. It is used to initiate the functionalities of Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark context\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sql context\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read CSV dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Todo: using SQLContext to read csv and assign to dataframe\n",
    "df = sqlContext.read.csv(\"adult.csv\", header=True, inferSchema= True)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Todo:printSchema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cell to rename the columns properly:\n",
    "cols = ['age','workclass','fnlwgt','education','education_num','marital','occupation','relationship','race','sex','capital_gain','capital_loss','hours_week','native_country','label']\n",
    "\n",
    "#note income -renamed-> as label\n",
    "df=df.toDF(*cols)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the continuous variable in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: float (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: float (nullable = true)\n",
      " |-- capital_loss: float (nullable = true)\n",
      " |-- hours_week: float (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital_gain', 'education_num', 'capital_loss', 'hours_week']\n",
    "# Convert the type\n",
    "df = convertColumn(df, CONTI_FEATURES, FloatType())\n",
    "# Check the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------+------------+-------------+------------------+-----------------+------------+-----+------+------------+------------+----------+--------------+-----+\n",
      "|age |workclass|fnlwgt  |education   |education_num|marital           |occupation       |relationship|race |sex   |capital_gain|capital_loss|hours_week|native_country|label|\n",
      "+----+---------+--------+------------+-------------+------------------+-----------------+------------+-----+------+------------+------------+----------+--------------+-----+\n",
      "|25.0|Private  |226802.0|11th        |7.0          |Never-married     |Machine-op-inspct|Own-child   |Black|Male  |0.0         |0.0         |40.0      |United-States |<=50K|\n",
      "|38.0|Private  |89814.0 |HS-grad     |9.0          |Married-civ-spouse|Farming-fishing  |Husband     |White|Male  |0.0         |0.0         |50.0      |United-States |<=50K|\n",
      "|28.0|Local-gov|336951.0|Assoc-acdm  |12.0         |Married-civ-spouse|Protective-serv  |Husband     |White|Male  |0.0         |0.0         |40.0      |United-States |>50K |\n",
      "|44.0|Private  |160323.0|Some-college|10.0         |Married-civ-spouse|Machine-op-inspct|Husband     |Black|Male  |7688.0      |0.0         |40.0      |United-States |>50K |\n",
      "|18.0|?        |103497.0|Some-college|10.0         |Never-married     |?                |Own-child   |White|Female|0.0         |0.0         |30.0      |United-States |<=50K|\n",
      "+----+---------+--------+------------+-------------+------------------+-----------------+------------+-----+------+------------+------------+----------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe Column by count,by group\n",
    "\n",
    "# To get a summary statistics, of the data, you can use describe(). It will compute the :\n",
    "\n",
    "* count\n",
    "* mean\n",
    "* standarddeviation\n",
    "* min\n",
    "* max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------+------------------+------------+------------------+--------+----------------+------------+------------------+------+------------------+------------------+------------------+--------------+-----+\n",
      "|summary|               age|  workclass|            fnlwgt|   education|     education_num| marital|      occupation|relationship|              race|   sex|      capital_gain|      capital_loss|        hours_week|native_country|label|\n",
      "+-------+------------------+-----------+------------------+------------+------------------+--------+----------------+------------+------------------+------+------------------+------------------+------------------+--------------+-----+\n",
      "|  count|             48842|      48842|             48842|       48842|             48842|   48842|           48842|       48842|             48842| 48842|             48842|             48842|             48842|         48842|48842|\n",
      "|   mean| 38.64358543876172|       null|189664.13459727284|        null|10.078088530363212|    null|            null|        null|              null|  null|1079.0676262233324| 87.50231358257237|40.422382375824085|          null| null|\n",
      "| stddev|13.710509934443518|       null|105604.02542315758|        null| 2.570972755592256|    null|            null|        null|              null|  null| 7452.019057655418|403.00455212435924|12.391444024252296|          null| null|\n",
      "|    min|              17.0|          ?|           12285.0|        10th|               1.0|Divorced|               ?|     Husband|Amer-Indian-Eskimo|Female|               0.0|               0.0|               1.0|             ?|<=50K|\n",
      "|    max|              90.0|Without-pay|         1490400.0|Some-college|              16.0| Widowed|Transport-moving|        Wife|             White|  Male|           99999.0|            4356.0|              99.0|    Yugoslavia| >50K|\n",
      "+-------+------------------+-----------+------------------+------------+------------------+--------+----------------+------------+------------------+------+------------------+------------------+------------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#todo: describe data,column by count,by group\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      capital_gain|\n",
      "+-------+------------------+\n",
      "|  count|             48842|\n",
      "|   mean|1079.0676262233324|\n",
      "| stddev| 7452.019057655418|\n",
      "|    min|               0.0|\n",
      "|    max|           99999.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the summary statistic of only one column, add the name of the column inside describe()\n",
    "\n",
    "df.describe('capital_gain').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   education|count|\n",
      "+------------+-----+\n",
      "|   Preschool|   83|\n",
      "|     1st-4th|  247|\n",
      "|     5th-6th|  509|\n",
      "|   Doctorate|  594|\n",
      "|        12th|  657|\n",
      "|         9th|  756|\n",
      "| Prof-school|  834|\n",
      "|     7th-8th|  955|\n",
      "|        10th| 1389|\n",
      "|  Assoc-acdm| 1601|\n",
      "|        11th| 1812|\n",
      "|   Assoc-voc| 2061|\n",
      "|     Masters| 2657|\n",
      "|   Bachelors| 8025|\n",
      "|Some-college|10878|\n",
      "|     HS-grad|15784|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Descriptive statistics by group\n",
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             marital| avg(capital_gain)|\n",
      "+--------------------+------------------+\n",
      "|           Separated| 581.8424836601307|\n",
      "|       Never-married|  384.382639449029|\n",
      "|Married-spouse-ab...| 629.0047770700637|\n",
      "|            Divorced| 793.6755615860094|\n",
      "|             Widowed| 603.6442687747035|\n",
      "|   Married-AF-spouse|2971.6216216216217|\n",
      "|  Married-civ-spouse|1739.7006121810625|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "df.groupby('marital').agg({'capital_gain': 'mean'}).show()\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosstab computation\n",
    "In some occasion, it can be interesting to see the descriptive statistics between two pairwise columns. For instance, you can count the number of people with income below or above 50k by education level. This operation is called a crosstab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----+\n",
      "|age_label|<=50K|>50K|\n",
      "+---------+-----+----+\n",
      "|     17.0|  595|   0|\n",
      "|     18.0|  862|   0|\n",
      "|     19.0| 1050|   3|\n",
      "|     20.0| 1112|   1|\n",
      "|     21.0| 1090|   6|\n",
      "|     22.0| 1161|  17|\n",
      "|     23.0| 1307|  22|\n",
      "|     24.0| 1162|  44|\n",
      "|     25.0| 1119|  76|\n",
      "|     26.0| 1068|  85|\n",
      "|     27.0| 1117| 115|\n",
      "|     28.0| 1101| 179|\n",
      "|     29.0| 1025| 198|\n",
      "|     30.0| 1031| 247|\n",
      "|     31.0| 1050| 275|\n",
      "|     32.0|  957| 296|\n",
      "|     33.0| 1045| 290|\n",
      "|     34.0|  949| 354|\n",
      "|     35.0|  997| 340|\n",
      "|     36.0|  948| 400|\n",
      "+---------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#todo crosstab computation\n",
    "df.crosstab('age', 'label').sort(\"age_label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop column\n",
    "There are two intuitive API to drop columns:\n",
    "\n",
    "* drop(): Drop a column\n",
    "* dropna(): Drop NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null vals\n",
    "df  = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing is a critical step in machine learning. After you remove garbage data, you get some important insights.\n",
    "\n",
    "\n",
    "**For instance, you know that age is not a linear function with the income. When people are young, their income is usually lower than mid-age. After retirement, a household uses their saving, meaning a decrease in income. To capture this pattern, you can add a square to the age feature**\n",
    "\n",
    "**Add age square**\n",
    "\n",
    "To add a new feature, you need to:\n",
    "\n",
    "* 1. Select the column\n",
    "* 2. Apply the transformation and add it to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: float (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: float (nullable = true)\n",
      " |-- capital_loss: float (nullable = true)\n",
      " |-- hours_week: float (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- age_square: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO:  # Add age square\n",
    "        \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1 Select the column\n",
    "age_square = df.select(col(\"age\")**2)\n",
    "\n",
    "# 2 Apply the transformation and add it to the DataFrame\n",
    "df = df.withColumn(\"age_square\", col(\"age\")**2)\n",
    "\n",
    "df.printSchema()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclude Holand-Netherlands**\n",
    "\n",
    "* When a group within a feature has only one observation, it brings no information to the model. \n",
    "* On the contrary, it can lead to an error during the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native_country|count(native_country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|             Hungary|                   19|\n",
      "|            Honduras|                   20|\n",
      "|            Scotland|                   21|\n",
      "|Outlying-US(Guam-...|                   23|\n",
      "|          Yugoslavia|                   23|\n",
      "|                Laos|                   23|\n",
      "|     Trinadad&Tobago|                   27|\n",
      "|            Cambodia|                   28|\n",
      "|                Hong|                   30|\n",
      "|            Thailand|                   30|\n",
      "|             Ireland|                   37|\n",
      "|              France|                   38|\n",
      "|             Ecuador|                   45|\n",
      "|                Peru|                   46|\n",
      "|              Greece|                   49|\n",
      "|           Nicaragua|                   49|\n",
      "|                Iran|                   59|\n",
      "|              Taiwan|                   65|\n",
      "|            Portugal|                   67|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO : follow the above instruction\n",
    "df.filter(df.native_country == 'Holand-Netherlands').count()\n",
    "df.groupby('native_country').agg({'native_country': 'count'}).sort(asc(\"count(native_country)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **The feature native_country has only one household coming from Netherland.** \n",
    "   -> we should remove it\n",
    "   -> follow the same logic for nay other column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove = df.filter(df.native_country!='Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a data processing pipeline\n",
    "You will build a pipeline to convert all the precise features and add them to the final dataset. The pipeline will have four operations, but feel free to add as many operations as you want.\n",
    "\n",
    "* Encode the categorical data\n",
    "* Index the label feature\n",
    "* Add continuous variable\n",
    "* Assemble the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encode the categorical data\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "# stages in our Pipeline\n",
    "stages = [] \n",
    "\n",
    "\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    \n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    \n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Index the label feature\n",
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx =  StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add continuous variable\n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Assemble the steps\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ebc07d1eb434,\n",
       " OneHotEncoderEstimator_4ed2f0346219,\n",
       " StringIndexer_817f9e246110,\n",
       " OneHotEncoderEstimator_9c9381de7575,\n",
       " StringIndexer_b7f3513b30eb,\n",
       " OneHotEncoderEstimator_f83a04fe496c,\n",
       " StringIndexer_c3029f6bd1b0,\n",
       " OneHotEncoderEstimator_37b8d38e4c4d,\n",
       " StringIndexer_2dbc48d794bb,\n",
       " OneHotEncoderEstimator_9a8100d19089,\n",
       " StringIndexer_83ba26e2d2bd,\n",
       " OneHotEncoderEstimator_59db38f2c633,\n",
       " StringIndexer_411249b200e4,\n",
       " OneHotEncoderEstimator_f573dd2b9bd4,\n",
       " StringIndexer_1f4c64eed576,\n",
       " OneHotEncoderEstimator_cff5b91a79ac,\n",
       " StringIndexer_ca12b56f23c6,\n",
       " VectorAssembler_20897a7265e3]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: float, workclass: string, fnlwgt: float, education: string, education_num: float, marital: string, occupation: string, relationship: string, race: string, sex: string, capital_gain: float, capital_loss: float, hours_week: float, native_country: string, label: string, age_square: double, workclassIndex: double, workclassclassVec: vector, educationIndex: double, educationclassVec: vector, maritalIndex: double, maritalclassVec: vector, occupationIndex: double, occupationclassVec: vector, relationshipIndex: double, relationshipclassVec: vector, raceIndex: double, raceclassVec: vector, sexIndex: double, sexclassVec: vector, native_countryIndex: double, native_countryclassVec: vector, newlabel: double, features: vector]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the classifier: logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the computation faster, you convert model to a DataFrame.\n",
    "#You need to select newlabel and features from model using map.\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "# from pyspark.ml.linalg import DenseVector\n",
    "df_train = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train/test set\n",
    "\n",
    "You split the dataset 80/20 with randomSplit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|       29671|\n",
      "|  1.0|        9315|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's count how many people with income below/above 50k in both training and test set\n",
    "train_data.groupby('label').agg({'label': 'count'}).show()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|        7483|\n",
      "|  1.0|        2372|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark has an API called LogisticRegression to perform logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You initialize lr by indicating the label column and feature columns. \n",
    "# Import `LogisticRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.06629458791643376,-0.15218373640701383,-0.053913615606758065,-0.16967312430449774,-0.12115314684082322,0.1325974961176333,0.1943887659566995,-0.6386553259560794,-0.20168892525823268,-0.06643613691435478,0.22587144074752571,0.3784635752251114,-0.0044245321889490345,-0.2958940967195082,-0.011315453541718349,-0.3286032800269526,-0.4220383458346703,0.5748881894870274,-0.40562097016057325,-0.2321967047902892,0.6030218476150906,-0.35292400176648736,-0.42080161545502637,0.32504577335055407,-0.3496608365399488,-0.20341828378529217,-0.21097882986337838,-0.16094910982880872,-0.10229984961820654,0.19354140559551283,-0.05741072218845116,0.27840022983220586,-0.10767152632715087,0.04564764837286497,-0.29067926822772944,-0.22290332501866694,-0.1707732587338623,-0.1265004208587058,-0.3046016757809549,-0.32401360966564163,0.11297915494496764,0.12425063820676888,-0.2716362961533152,0.26838077428027984,-0.1980362522010291,-0.29122621595643805,-0.24230441670165628,0.4124628047635877,-0.05433645026419642,-0.1908446548558885,-0.06449555808439705,-0.269348178043779,0.1680087916932683,-0.1253754753434063,-0.3868581630578742,-0.18369433515375264,0.02912182143548542,-0.08956223920758943,-0.2708959392074213,-0.03487727693207883,-0.3241391915336244,-0.04758250791733353,-0.1173475422607489,0.08015333921255102,-0.36180606787408326,-0.33618210864537906,-0.16633535059376112,-0.006091174986221248,-0.45442144073339164,-0.05071249102702611,-0.3521378742759532,-0.2426920862206138,-0.21721353897728612,-0.5821780298068088,-0.17111959024994547,-0.00021772139996622131,-0.15792241012962724,-0.13336347352772165,-0.12444849686729739,-0.4299324718213068,-0.4802401343425621,-0.3509031990149178,0.15554349625772512,0.04444256440286982,-0.2897702852575532,-0.19176517240952143,0.35513884661883627,-0.47121791074628866,0.16608265034986552,-0.6627801214856749,-0.4078693147975695,-0.22434028566679146,-0.21468806117681136,0.007013088722353848,1.1642146321728235e-07,2.1439613693010857e-05,0.028523559404261892,0.00022176143684426227,0.008705658401723285]\n",
      "Intercept: -2.0325289129477486\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate prediction for your test set, you can use linearModel with transform() on test_data\n",
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.91956529713708...|\n",
      "|  0.0|       0.0|[0.93120436510470...|\n",
      "|  0.0|       0.0|[0.92517755280318...|\n",
      "|  0.0|       0.0|[0.96557850534464...|\n",
      "|  0.0|       0.0|[0.91327749860183...|\n",
      "|  0.0|       0.0|[0.89372112927668...|\n",
      "|  0.0|       0.0|[0.91170827845144...|\n",
      "|  0.0|       0.0|[0.79855876724043...|\n",
      "|  0.0|       1.0|[0.33707064577134...|\n",
      "|  0.0|       0.0|[0.93124041985776...|\n",
      "|  0.0|       0.0|[0.65819871502704...|\n",
      "|  0.0|       0.0|[0.68540875943836...|\n",
      "|  0.0|       0.0|[0.87432677309934...|\n",
      "|  0.0|       0.0|[0.81416508490888...|\n",
      "|  0.0|       1.0|[0.41896286779831...|\n",
      "|  0.0|       0.0|[0.55828647666212...|\n",
      "|  0.0|       0.0|[0.57431564825875...|\n",
      "|  0.0|       0.0|[0.53485051662803...|\n",
      "|  0.0|       0.0|[0.80318175590664...|\n",
      "|  0.0|       0.0|[0.87915048489301...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 81.887%\n"
     ]
    }
   ],
   "source": [
    "#We need to look at the accuracy metric to see how well (or bad) the model performs.\n",
    "\n",
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "\n",
    "accuracy_m(model = linearModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894260182552082\n",
      "areaUnderROC\n"
     ]
    }
   ],
   "source": [
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## many new metrics libraries are now introduced in pyspark .\n",
    "check https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can tune the hyperparameters. \n",
    "#Similar to scikit learn you create a parameter grid, and you add the parameters you want to tune. \n",
    "#To reduce the time of the computation, you only tune the regularization parameter with only two values.\n",
    "#use \n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 1079.451 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 84.820%\n"
     ]
    }
   ],
   "source": [
    "#accuracy of cv selected model\n",
    "accuracy_m(model = cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_c188ddedd32b', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0,\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto',\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='maxIter', doc='maximum number of iterations (>= 0)'): 10,\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='regParam', doc='regularization parameter (>= 0)'): 0.01,\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5,\n",
       " Param(parent='LogisticRegression_c188ddedd32b', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can exctract the recommended parameter by chaining cvModel.bestModel with extractParamMap()\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
