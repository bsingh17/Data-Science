{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7346e67499d9a00dd998f07f827fefbbaa0a790f"
   },
   "source": [
    "# INTRODUCTION<br><br>\n",
    "**In this kernel, we will see Natural Language Processing(NLP).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "### Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55c9c1c04fdff1b3caa43360cf6c4107d9d0beb4"
   },
   "outputs": [],
   "source": [
    "# import twitter data\n",
    "data = pd.read_csv(\"../input/gender-classifier-DFE-791531.csv\",encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5568b807326ba6e77a145df77f2b5d3db3220c71"
   },
   "outputs": [],
   "source": [
    "#Concatenate the data attributes gender and description\n",
    "\n",
    "data = pd.concat([data.gender,data.description],axis=1)\n",
    "\n",
    "#Drop the NaN values\n",
    "\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4e16dfc57c63f82d2cc144214eecdf1513981e20"
   },
   "outputs": [],
   "source": [
    "# Inspect  the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3deff8f51f6d47d8b3d94af084d0380548460916"
   },
   "outputs": [],
   "source": [
    "#Retrieve the shape of the dataframe \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eee1cd412e68655cf49aea32884812249a9753bf"
   },
   "outputs": [],
   "source": [
    "#Define the data gender variable (1 for female and 0 for male)\n",
    "\n",
    "data.gender = [1 if each == \"female\" else 0 for each in data.gender]\n",
    "\n",
    "#Inspect the data again(take 10 values)\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9976f3ec86ac2c0044fec29ba60040c09edd7891"
   },
   "outputs": [],
   "source": [
    "# Retrieve any one description \n",
    "# We will first employ the cleaning of the text for one description and\n",
    "# then clean all the descriptions\n",
    "\n",
    "data.description[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a6706de5d839df851df94d365a91b07af9ad035"
   },
   "source": [
    "### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc1ce1b0524b4dc502a78373915e31add3aca563"
   },
   "outputs": [],
   "source": [
    "# Regular expression to be used RE =>> \"[^a-zA-Z]\"\n",
    "#Import the regex library\n",
    "import re\n",
    "\n",
    "# Use re.sub to filter out the text from the raw data\n",
    "# Also make sure all the text is lower case.\n",
    "\n",
    "first_description = data.description[4]\n",
    "description = re.sub(\"[^a-zA-Z]\",\" \",first_description)\n",
    "description = description.lower() #Year year are different words\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "728a1a984c18d51f3fb3a82c35c879c1331391a5"
   },
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df2ff835d44c1b28f2f526a7be90cbe189a0c8f3"
   },
   "outputs": [],
   "source": [
    "#Use nltk libraries to remove irrelevant words known as stopwords\n",
    "#Import the libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#remove irrelavent words for e.g. and,the ...\n",
    "#Tokenize the text using word_tokenize \n",
    "#You can also use .split function and then tokenize the words\n",
    "#if we use word_tokenize instead of split it will be better\n",
    "#split() = shouldn't => shouldn't\n",
    "#word_tokenize() = shouldn't => shouldn't and n't separate as two word\n",
    "#Tolenize the text\n",
    "\n",
    "description = nltk.word_tokenize(description)\n",
    "\n",
    "\n",
    "# Filter the stopwords from the text and print the filtered text.\n",
    "\n",
    "\n",
    "description = [word for word in description if not word in set(stopwords.words(\"english\"))]\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "baf7ac67dd99d4862d05b6cb2843cb8581851529"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5327f3b78cf09fee2fa7be234853c119bcd6f47"
   },
   "outputs": [],
   "source": [
    "#Lemmatization = loved => love\n",
    "\n",
    "#import the nltk library\n",
    "import nltk as nlp\n",
    "\n",
    "#Employ lemmatization to the text\n",
    "\n",
    "\n",
    "lemma = nlp.WordNetLemmatizer()\n",
    "description = [lemma.lemmatize(word) for word in description]\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65a2a1f65e76a45ee08d25679ddcb4a27966d537"
   },
   "outputs": [],
   "source": [
    "# Rejoin the tokenized words into a corpus and print the cleaned text\n",
    "\n",
    "description = \" \".join(description)\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfc912fb19c632215d35352e53502ca2287a9149"
   },
   "source": [
    "### Apply to All Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a28c91ac696d580d821bc4aee40d860ba43f69e4"
   },
   "outputs": [],
   "source": [
    "#Apply everything you did above to all the remaining descriptions\n",
    "\n",
    "description_list = []\n",
    "for description in data.description:\n",
    "    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n",
    "    description = description.lower()\n",
    "    description = nltk.word_tokenize(description)\n",
    "    #description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
    "    lemma = nlp.WordNetLemmatizer()\n",
    "    description = [lemma.lemmatize(word) for word in description]\n",
    "    description = \" \".join(description)\n",
    "    description_list.append(description)\n",
    "    \n",
    "#description_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a69d229dc6ae19d4bc78a6a87e0afea1a029483d"
   },
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a311d5f174ab11f38cf82623913f811f2c8a6d57"
   },
   "outputs": [],
   "source": [
    "#Import the sklearn feature extraction from text library to vectorize the count\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define max_features as the maximum number of features you want to employ.\n",
    "# Use the count vectorizer after defining a specific number as max_fatures\n",
    "max_features = 1000\n",
    "count_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n",
    "#count_vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "#Define the sparce matrix\n",
    "sparce_matrix = count_vectorizer.fit_transform(description_list).toarray() # x\n",
    "\n",
    "# Print the most common words\n",
    "\n",
    "print(\"{} most common words: {}\".format(max_features,count_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d9484f080c638da7eff571b380195df8b2eef4f"
   },
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "736a349b318334b1d4f57045d883a3e4234a0b9e"
   },
   "outputs": [],
   "source": [
    "#Define the x and y variables\n",
    "\n",
    "y = data.iloc[:,0].values   # male or female classes\n",
    "x = sparce_matrix\n",
    "# train test split (80-20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "782d1f2b0e2cc5666b53646e67a5caf82e0b2496"
   },
   "source": [
    "### Apply Naive Bayes Machine Learning Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff47650b02e92473b676c2ca812f6561a629cbbb"
   },
   "outputs": [],
   "source": [
    "#Apply Naive Bayes\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(x_train,y_train)\n",
    "\n",
    "print(\"accuracy: \",nb.score(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ee183271e64d6d1bb6c5e57ac85081ca777c8a3"
   },
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix \n",
    "\n",
    "y_pred = nb.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_nb = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "sns.heatmap(cm_nb,annot=True,cmap=\"RdPu\",fmt=\".0f\",cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e93373995e666949e6ed874f9ade07323ca1bad9"
   },
   "source": [
    "### Apply Random Forest Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8acc313de9b26f7b2e421dc25285069010121fa8"
   },
   "outputs": [],
   "source": [
    "# Apply Random Forest Machine Learning Algorithm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "rf.fit(x_train,y_train)\n",
    "\n",
    "print(\"accuracy: \",rf.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa005475ccbb24e2884a6ba8f9521d73d904be62"
   },
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix for Random Forest\n",
    "\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_rf = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "sns.heatmap(cm_rf,annot=True,cmap=\"RdPu\",fmt=\".0f\",cbar=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
