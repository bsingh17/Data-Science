{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Multilayer Perceptron (from Scratch)\n",
    "\n",
    "In this project you will implement a Multi - Layer Perceptron neural network from scratch using the iris dataset.\n",
    "You will initialize weights for forward propagation and perform gradient decent for backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries\n",
    "These are the packages that you will be using within this Neural Network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Reading the dataset\n",
    "Modify the parameters to correlate with the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mention path\n",
    "path=\" \"\n",
    "\n",
    "# load the dataset\n",
    "iris=pd.read_csv('path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the percentage of missing data contains in all the columns\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data looks really clean. So you do not need any imputation and you can start our EDA and model implementation with this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA Analysis\n",
    "\n",
    "* Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n",
    "\n",
    "* It is a good practice to understand the data first and try to gather as many insights from it. EDA is all about making sense of data in hand,before getting them dirty with it.\n",
    "\n",
    "* There are 3 types of EDA Analysis, we will do them all step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Uni-Variate Analysis of the Data\n",
    "\n",
    "* “Uni” means one and “Variate” means variable hence univariate analysis means analysis of one variable or one feature. Univariate basically tells us how data in each feature is distributed and also tells us about central tendencies like mean, median, and mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the Distribution of speicies\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Bi-Variate Analysis**\n",
    "\n",
    "* Bivariate analysis is the simultaneous analysis of two variables (attributes). It explores the concept of relationship between two variables, whether there exists an association and the strength of this association, or whether there are differences between two variables and the significance of these differences. \n",
    "\n",
    "* Lets analyze all the features using boxenplots and strip plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sepal_length vs species boxenplot\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code\n",
    "\n",
    "\n",
    "# sepal_length vs species strip plot\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sepal_width vs species boxenplot\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code\n",
    "\n",
    "\n",
    "# sepal_width vs species strip plot\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# petal_length vs species boxenplot\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code\n",
    "\n",
    "\n",
    "# petal_length vs species strip plot\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# petal_width vs species boxenplot\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code\n",
    "\n",
    "\n",
    "# petal_width vs species strip plot\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Variate Analysis\n",
    "\n",
    "* Multivariate analysis (MVA) is based on the principles of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. Typically, MVA is used to address the situations where multiple measurements are made on each experimental unit and the relations among these measurements and their structures are important.\n",
    "* Essentially, multivariate analysis is a tool to find patterns and relationships between several variables simultaneously. It lets us predict the effect a change in one variable will have on other variables. ... This gives multivariate analysis a decisive advantage over other forms of analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze all the correlating features using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a heatmap\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Pre-Processing\n",
    "Pre-process the data so that it can smoothly run through the Multilayer Perceptron.\n",
    "Process includes:\n",
    "1. Establishing numerical values to the label vector (y) using Sklearn's One Hot Encoder. The One Hot encoding provides a binarization of data so that the neural network does not assume a heirarchy between the classes.\n",
    "\n",
    "2. Split the data into training and testing sets for both the feature vector and label vector.\n",
    "\n",
    "3. Create total number variables for feature and label vectors in order to calculate random initial weights and biases.\n",
    "\n",
    "4. Create initial weights and biases as a starting point for the neural network. We'll need a starting point for the weights and biases so that the forward propogation process can begin with these initial values. The values will later be optimized through the backpropogation process (that is where the learning takes place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset up into feature and label vectors\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code\n",
    "\n",
    "# learning rate and initial lambda initialization\n",
    "lr = .0001\n",
    "init_lam = .01\n",
    "\n",
    "# modify the epochNum, or number of iterations that our network runs\n",
    "epochNum = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Sci-Kit's one-hot encoder to transform the classes into numerical values\n",
    "# to feed into our multilayer perceptron\n",
    "numVal = array(y)\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(numVal)\n",
    "\n",
    "# encode from integer to binary\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# splitting up training and testing data into appropriate vectors for initial \n",
    "# group\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, onehot_encoded)\n",
    "\n",
    "# establish the number of labels and features\n",
    "num_labels = y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# set size of hidden layer\n",
    "hidden_nodes = 3 * len(X_train)\n",
    "\n",
    "# initializing the arrays of weights and biases\n",
    "# complete the code below\n",
    "\n",
    "w1 = None \n",
    "w2 = None \n",
    "\n",
    "b1 = None\n",
    "b2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functions for the neural network\n",
    "These functions include:\n",
    "\n",
    "### * relu_activation -->**\n",
    "I have chosen the relu activation function as it has a simple implementation and is often very accurate as it solves the vanishing gradient issue with other activation functions. It simply takes the max of 0 and the input vector x. (i.e. max(x, 0))\n",
    "\n",
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAARoAAACzCAMAAABhCSMaAAABAlBMVEX////Ly8tpaWnu7u7l5eX19fXv7+/h4eHr6+va2tqHh4d2dnb8/PzOzs739/fT09MoKP7Ly/7BwcFubm6Xl5etra0AAAB+fn5SUlLAwMBycnKjo6NmZmZZWVmzs7MAAP4kJCRCQkKMjIxKSkpeXl6UlJSmpqY7OzszMzP19f/Jyc7R0f/t7f+7u/////jExP8bG/+iotulpf9hYf6zs//a2vqcnP9JSf6Cgv+Kiv5bW/54eP/Y2P80NPshISEXFxfr6+GQkOWYmOSJifNPT/1vb/47O/nHx+Giov7BwdHn5/PV1exaWvjd3dByctq1tf1AQOtoaOI4OP2srMZKSuC6uuP+TY0nAAANgUlEQVR4nO2dC3+aShqHX+QOAyRcAohgVFDTJK2bNM2lzWl30/ac7Tlne7a7+/2/ys6gRuUW0yQqOv9fG8F3LvA4M8wdACoqKioqKgBNWDyXpaW8CdoLXMuqZLhugsaHYoT/ODIgJ71vpHDj77kAwJxDoQc8GNxiMLar4m8XZRogmS9z1SuRo+iKAiCJAjAWPrdM0FsisfQPHPyXE3mzI+kCkhAgAXHYndhhBF4HnpMASSynE8c9RtcRxieBwHIsxsdx4NrYHyCOQyAI3HLJbJMU2dCPgQmTUNJUfN7EaKwUTWx7mJzXDZzDhO0ZsQmOzVtJT/QPFUaJhLDbY6ShYrnEcYi9SD3Qe4Lb6/Y43nJdrtfqMz3oW9iFEnY74lrv8ycUHcZDA2ID7MBYQKP1IJTYjg7IwIg8g1EgZECHwNFDFhQ/coFpCQMEHZIeOr3QEULQQ8ENwPZf4YQIdgRaKPRY8LhuAt1gvTf6eOFU40YQNq0mk6YaNZpkqCTWWiqHqYBG0Gi6hxHJnuXZQihhNI4DLL5xnJeIa5JqCBpPwKFFTkDyYkLQcB6CBtP1wbbXep8/IacLXI8AQQLTJOcWyCQdsB3bcTo4IwEn9wga6HYCUBKwbN4jaEwPApXt4ARD0HgG/jMUmMEYjRHyAsJ8NA95hjRESlRDNH4f8H2CG7ZkuRd6ASRhizxxZQU/t1SGi70+KD3RlUHDmUuKY9vBBZDTZyAILV3CNFXysHIJn8jrKrxtgonpxCHHxU3DBbkVY7cMOM6a75SKiorqBaS2GlN5jVJtvilsldtm3mLlMWhmh0a5qw0xyeWmqpryLET0k2iYclerNFU0tQ1UapL1ZSLjq9Hovo/btNGklUvRzEkIOsCo06rlLqCZhfgAGtBjUAUgbRuQ5FgQ2LEEc3qU00pNTIVJKrMJWplJEM7+xk+PuQfQ8CE0EYTpzxB54lScz4klWqnJLLNUmhi5xPLp7+1/3Ecmdx9MNS6HwvHJ1meo473TpTOU7hyaZkOZdH5sO5rj9tvli2FkGIYuG5OgthzN6/bdI55Qi9puNEe3V0DRFKFB5+dHQNEUoTn/fLIYGUUzFrq+PclERtGM9bZ9kY2Mokl12j7ORUbREH1on+Yjo2iwLkbvCiKjaABObq9mbimaOR2d38w5pWhmQl8u511SNPd6fTc6KY5s59G8ay+QoWjuD9+235RFtuNo9vdOMzaKZqz9vbPyyHYazVH7uiKyZdAgUWbTgy1Dc/T1+nVFZMugkVWnmQ7SbRcadH5Z4ORxaLQu30qTzXah+XJzUmB7HBrOUrs4HjnyuHuZXKk2xMSI1aar0a+fKkOUl0DjRGAZmJARS1OxJiuVaKUmpsJUZpFYjWNfv2sff6qMjBWXQBMpRiOdvb1FGWr/vu+qNLKlnlC+M56ssjVoXs/1XZVGtpv1mt/bvz0c2U6i+XXvutS422iObj6XY9tpNOj8azmZ3UZz/eOfFE2hztr8s8zlW9Q2oPne3n+eaY6L2gI0+6M3zzQDdFH1R7M/egsUTRGak9Ed+aBocl+dXF6lnxRN7qvPN+NePYom+8193xVFk/ni6nbaq0fRLJ7/NuuhoWgWTk9xVW8qimb+bH++74qimTv51r6bO3tGNGLEpIHVFs3J3jyZZ0QjuP6rWg/RHX2+Wchdz4fG6IpkFyJJbglT8SYvlGilJqbMIpClYlNXNzfSgkmTcq4LIntoqRhR5PWbLCbk9GaLqvzShVgrNZWvB5uZ5Pdff180lS4VW4jMeGCpGNErG5Q0ndUzQ71rf8uYnpKhdODY2RmrOFZ9h+jeTKfYz/QENKyj96y5govTxtt41RHNcft7zvQENFLX1rrZHcyglmj2C8g8BY2utNB2oLkY3RWYfh4NjziOLdp3oXZopn1XGf08mkhSXVct6D+sG5qjmz8KTU/IUAg/nuQtQHP1o2je1ZPQCLHvhPVHc7V3VGx6Sr3m1cGwaKfIWqH5892oZHbRk+o1LZEJC7zXCs1Zdor9TE8pa/CD2645mv1RQYVmoqdkKFEJ3XqXNSejf+WnS0/182j8npvuQplTfdB8u73TXmLXIz92/XqjQZ/PX2pDKC4Jm3XOUDdfjl5wryyzvmjQ9eXJC20jVhpmTdCMV1O+CJq+zea/JKoFmtffx31XL4JG95tOFNU1Qx1Pqnovk6G0uGsnNUVzPJqsGXwRNI6SeW0BRGMPNUAz2x7iRdAI2UCNcDzusPloTi7fTw9feqPLiZuIoJH9jV8q9unzX/fHDy0VezDEZZaK+Y6WABlX2MClYgtn3L8vv7GFpkVfGldum0W2zFIxu9UbpF3Fm56h7ub7rlaSobD7sasNR/Pb6MPc2YrQ6OORl81G83200He1IjQTbTSaN+23CyaKZqr5/a5SUTQTXYyyqykpmrHQ5Xm2v5OiSYXOz3ODcRQNEbr6nB+Mo2iIrke52UUUTYrmLD/vCigagua0eHsIigYuMlW9qSiai1FuJ7Cxdh2NcfL1fZlpx9H8/vW8zLTraK5+lMwu2nk0V6Py+99tNGd7/yn3tdNoTtv7Ve9iWwUayfc3cTfHN6O363yr2DikyB/P0dooNCekh2bdaHDTNk7fWO/Ovqm4ptWYPuy9q/ZV9XLDpV64pi+Fxu7jP4zdke/ly6Vajenyv39W+zIrTMYykWlLLBWDKOXHSw1e58fSmelRTqsw6fwf59/01FTmTdf4UpPBLhGZLi2RapgDZ8NW7KL349fyrL2s4TTN2Cw013uTHpp1o7nXpqCZDVNSNIt6M9segqJZ0PHc9hDLo+HxKc+OB6gnaPQiQnVGczG/i/3SaKQOC8bQG6S1nQkap1EZWe3QHN3O910tXeUj0+c9W3c8QmXyPmu+U5Bs6ovm6PbLfGrIowk97yDpDw8M0RoOQwmSHggHJngayAME0iFZ56X9EkF/oINqV0VWMzTo/eXCMGUeTcsTzY+uZLcMQ+C7Cui9vusCOpSBGQKwAzKDU/aHxqFRnKNqi+b97WKvXgEaG6ShCGLPdHq9gYdLmYMhAv2jCCZB0yHTy2RkH5B34zIFqyhriub12SgztF2AJgBpwALX6R+aXNQjaEL8MDrkwBjg2usvJEMRNCRQzcsXNjVFkx+MK0Uj9roYi4L/N5QwAPRRBjQw8UOKPL5lYxANcc6KWlXXUSc0b/J7tRcUw3Za1orDqBNbwx5gQuJHDWITkxi6H32QfpGNAx/cFoLmthTDF/nX8hSg0UTArWfgGU32TZFBDE4dpggO6WtifOxePsSf+MZ9pA8KliXXEc2HvYLtIZau17DetMRlguny/sKlgjVEc/SjaJhy+YaCPLeQaVIbTre5Kr+OuqA5Oj8v2BqFNi/Tql7hMCVFA9e3RbOLKBoG3u19KDatGw3y7bW+zubP09LtIdaNRnQNNds3/ApQiZ7ddHI1Kp53BetHE0TQNBaWin06a++VavTMpvZf/yt6/ReRVL6KTGLKl4NVLBWTTOkxS8UwGhejEZnwfkXVxd/2S/XcpmNfKF3ZZZZapPKlYpjacy0Vi/rQyC4VW/vA7oOmlWQo1lKS9GDdLe+81o0GBG4cFkVTKoqmVHNoqqZDbYZJLjcVbceYDxE9Bk3QakxkHVqNEq3WVGZpWMNWqWkQl3ubRdZyHoFmTtZmmNRyU8Vvbhc14h+ObEnVAE15gZIUbTr4cGRLakMKlKqyprwYFiuK4YoQqZ4g+yMunzyP9MZLnpcpq/pWh4xgqHErl2iDgWXiDzP0cg+PqOX5JGSvkTWpIckz3dDNFQ9Sq4FNfBy7+exxaJF80Q+tnIl31QMfkOqpmZicYQCBFxMPSlgwCLOUdNymMrtAJoXaPh9mzTa5/2ZBkg3I3QNqCmZu/iALMunCtnPMuAaoBogWJLkaDG6tt0Tgi2KC9KIEi4+iIiPCz+Xc85tnAmghxyHMwa2oSpWINEhZ0tzsjlucCgszNKTJKgGbDpYmcXP+B2OxSXC8Fr51tqnL8z+KQHwhCEgidEIrA8dMwHZAUyAK8lcjxWQGa1ywn3bYws8nTkVFkzgZF2NNQje7H5gZ4CJYxJeBY3MKYntAgW3bPkFjR0BSpCLNoYmwMZpcDQJnfvjLJyYBCBPBQsY8GhObHGRaeurLnJuanN6HAkkEBkaT//15dZySEr/gSl0TuKbOFIzBJWb6kbt7jKYBMvZguBD0SwA8JIxGbuqEiWPL2WzZJTEjHYIceAE0kmtc2clFLIZpGtPBz9RGUIhaHE7ivJuv3DadcUxKLvXjLy0GeIuz89R0HB7Z/zXIVuh8nKG4LvHg6Y2q+nKFoqbqg90wwBEhaWYDsSVANgTNJFcGRE1XwCZJVXI5wHHVBALkNLvZoaGoEXEOMJaTexRzquuaDt9tBrmYcFnb5/v498ubgHWA7fN2M1vt85vNCHsgdrPxk/VgKiqqGgplNiNFFW+a3jHFMpdOmuqG4wJTq+iA2TFxYTN9KseuyNvgy75oqO5DnnZEbjr9xUxwvQ3XF/REazEV/Sw7pU6D1OUVy+0AOnAgYRi3W9GbsENyGdJqkBoy/mcHqpAwHB/T3hQsDjc0Agk43HYycKODMUwxUB7f6qOioqKiotpu/R/UV1HZGBzwFwAAAABJRU5ErkJggg==\"\n",
    "width=\"500px\">\n",
    "\n",
    "### * softmax -->\n",
    "The relu activation function only works when used as an activation function on the hidden layers of a neural network, therefore I have chosen to use the softmax activation function for the output layer. The softmax function conveniently places the output values within the range [0, 1]. Since all the probabilities that will go towards the output layer will equal 1, the softmax activation function will choose the classification with the highest probability. Easy as that!\n",
    "\n",
    "<img src= \"data:image/png;base64,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\"\n",
    "width=\"500px\">\n",
    "\n",
    "\n",
    "\n",
    "### * forward -->\n",
    "The neural network will feed forward the input data. The data will be computed at each layer using f(x) = wx + b function as w = weight values, x is input values, and b is the bias of the layer. Then after that value is calculated at each layer, it will be put through the layer's associated activation function to keep moving forward through the neural network.\n",
    "\n",
    "### * backprop -->\n",
    "This is the step of the process where the neural network learns, as it identifies the error of the output nodes. This establishes a loss value and a gradient or slope for that value at each node. When the data is fed back through the network, each value goes through gradient descent where the local minimum of its specific gradient is calculated so that it can adjust new values to the parameters of the neural networks to minimize loss the next time data is fed through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective activation function of my choosing\n",
    "# complete the relu_activation function and return the output of the function\n",
    "def relu_activation(vec):\n",
    "    \n",
    "    ## start code\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## end code\n",
    "\n",
    "\n",
    "# returns a vector of output probabilities\n",
    "# initialize input and output for the softmax function\n",
    "def softmax(vec):\n",
    "    # for softmax we compute input over number of choices\n",
    "    input = None\n",
    "    # output is sum of all of those choices, K\n",
    "    output = None\n",
    "    return input / output\n",
    "\n",
    "\n",
    "def forward(softmax_vec, onehot_labels, lam, w1, w2):\n",
    "  \n",
    "    # first we calculate softmax cross-entropy loss (refer to formula)\n",
    "    i = np.argmax(onehot_labels, axis = 1).astype(int)\n",
    "  \n",
    "    # since softmax output will be probability values (non-integer) we use function\n",
    "    # arange() \n",
    "    predicted = softmax_vec[np.arange(len(softmax_vec)), i]\n",
    "    logs = np.log(predicted)\n",
    "    loss = -np.sum(logs) / len(logs)\n",
    "    \n",
    "    # second we add regularization to the loss in order to avoid overfitting\n",
    "    w1_loss = 0.5 * lam * np.sum(w1 * w1)\n",
    "    w2_loss = 0.5 * lam * np.sum(w2 * w2)\n",
    "    return (loss + (w1_loss + w2_loss))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(w1, b1, w2, b2, lam, lr, output_vec, hidden_vec):\n",
    "    output_error = (output_vec - y_train) / output_vec.shape[0]\n",
    "\n",
    "    hidden_error = np.dot(output_error, w2.T) \n",
    "    hidden_error[hidden_vec <= 0] = 0\n",
    "\n",
    "    gw2 = np.dot(hidden_vec.T, output_error)\n",
    "    gb2 = np.sum(output_error, axis = 0, keepdims = True)\n",
    "\n",
    "    gw1 = np.dot(X_train.T, hidden_error)\n",
    "    gb1 = np.sum(hidden_error, axis = 0, keepdims = True)\n",
    "\n",
    "    gw2 += lam * w2\n",
    "    gw1 += lam * w1\n",
    "\n",
    "    # update the weights and biases\n",
    "    \n",
    "    w1 -= None\n",
    "    b1 -= None\n",
    "    w2 -= None\n",
    "    b2 -= None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training our Model\n",
    "Here, we establish the amount of iterations that we will feed the data through the network in order to train it and prepare it to make predictions on new data. epochNum in this case is the amount of iterations that we will initialize.\n",
    "\n",
    "In each iteration, there are a few things that are happening:\n",
    "\n",
    "1. The values at each layer are being established with respect to that specific layer's activation function.\n",
    "\n",
    "2. The softmax activation function (in this case) will make a decision on the classificiation it believes is correct from choosing the classification with the highest probability (since the softmax outputs values between [0, 1].)\n",
    "\n",
    "3. With these established vectors at each layer, we will first feed the data forward through forward propogation to get our output values\n",
    "\n",
    "4. Then we will adjust our network's parameters in the process of backpropogation so that the network can classify more accurately in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we need to return the object 'epoch' in this case we will use xrange()\n",
    "# rather than range() function in python\n",
    "\n",
    "for epoch in range(1,epochNum):\n",
    "    # wx + b\n",
    "    input = np.dot(X_train, w1) + b1\n",
    "    hidden = relu_activation(input)\n",
    "    output = np.dot(hidden, w2) + b2\n",
    "    soft_output = softmax(output)\n",
    "\n",
    "    forward(soft_output, y_train, init_lam, w1, w2)\n",
    "    backprop(w1, b1, w2, b2, init_lam, lr, output, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. It is time to test the network!\n",
    "\n",
    "I have defined an eval() function that will put the network's prediction vector up against the actual classes of the data that it was fed. This function pretty much just calculates the correctness by dividing the correct predictions against the total number of rows of data the network evaluated, or total number of predictions it had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "def eval(preds, y):\n",
    "    ifcorrect =  np.argmax(preds, 1) == np.argmax(y, 1)\n",
    "    correct_predictions = np.sum(ifcorrect)\n",
    "    return correct_predictions * 100 / preds.shape[0]\n",
    "  \n",
    "\n",
    "input = np.dot(X_test, w1)\n",
    "hidden = relu_activation(input + b1)\n",
    "scores = np.dot(hidden, w2) + b2\n",
    "probs = softmax(scores)\n",
    "print('Accuracy of Multilayer Perceptron: {0}%'.format(eval(probs, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[:10],y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since  predictions and target variable is still encoded so to compute the confusion matrix and classification report we have convert them to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_test and probs to integers\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# print confusion matrix\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code\n",
    "\n",
    "\n",
    "\n",
    "# print classification report\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the hyperparameters like learning rate, no. of epochs, etc. in code above to observe the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your conclusions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
